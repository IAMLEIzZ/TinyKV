# 项目3 MultiRaftKV

在项目2中，你已经基于Raft构建了一个高可用的kv服务器，干得不错！但这还不够，这样的kv服务器由单个raft组支持，而单个raft组的扩展能力是有限的，并且每个写请求都会等待提交后依次写入badger，这是确保一致性的关键要求，但也扼杀了并发性。

![multiraft](imgs/multiraft.png)

在本项目中，你将实现一个基于多raft的kv服务器，并带有平衡调度器。该服务器由多个raft组组成，每个raft组负责一个键范围，这里称为region。布局如上图所示。单个region的请求处理方式与之前相同，但多个region可以并发处理请求，从而提高了性能，但也带来了一些新的挑战，比如如何平衡每个region的请求等。

本项目分为3个部分，包括：

1. 在Raft算法中实现成员变更和领导权变更
2. 在raftstore上实现配置变更和region分裂
3. 引入调度器

## 第一部分

在这一部分中，你将在基本的Raft算法中实现成员变更和领导权变更，这些功能是后续两部分的基础。成员变更，即配置变更（conf change），用于向raft组中添加或删除节点，这会改变raft组的法定人数，因此需要小心处理。领导权变更，即领导权转移（leader transfer），用于将领导权转移给另一个节点，这在平衡中非常有用。

### 代码

你需要修改的代码主要集中在`raft/raft.go`和`raft/rawnode.go`中，同时可以查看`proto/proto/eraft.proto`以了解需要处理的新消息。配置变更和领导权转移都是由上层应用触发的，因此你可能需要从`raft/rawnode.go`开始。

### 实现领导权转移

为了实现领导权转移，我们引入两种新的消息类型：`MsgTransferLeader`和`MsgTimeoutNow`。要转移领导权，首先需要在当前领导者上调用`raft.Raft.Step`并传入`MsgTransferLeader`消息。为了确保转移成功，当前领导者应首先检查被转移者（即转移目标）的资格，例如：被转移者的日志是否是最新的等。如果被转移者不符合条件，当前领导者可以选择中止转移或帮助被转移者，既然中止没有帮助，我们选择帮助被转移者。如果被转移者的日志不是最新的，当前领导者应向被转移者发送`MsgAppend`消息，并停止接受新的提案，以防我们陷入循环。如果被转移者符合条件（或在当前领导者的帮助下），领导者应立即向被转移者发送`MsgTimeoutNow`消息。收到`MsgTimeoutNow`消息后，被转移者应立即开始新的选举，无论其选举超时时间如何。由于拥有更高的任期和最新的日志，被转移者有很大机会让当前领导者下台并成为新的领导者。

### 实现配置变更

你将在这里实现的配置变更算法并不是扩展Raft论文中提到的联合共识算法，该算法可以一次性添加和/或删除任意节点。相反，它只能逐个添加或删除节点，这更简单且易于推理。此外，配置变更从调用领导者的`raft.RawNode.ProposeConfChange`开始，该调用会提议一个条目，其中`pb.Entry.EntryType`设置为`EntryConfChange`，`pb.Entry.Data`设置为输入的`pb.ConfChange`。当类型为`EntryConfChange`的条目被提交时，你必须通过`RawNode.ApplyConfChange`应用它，并使用条目中的`pb.ConfChange`，然后才能根据`pb.ConfChange`通过`raft.Raft.addNode`和`raft.Raft.removeNode`添加或删除节点。

> 提示：
>
> - `MsgTransferLeader`消息是本地消息，不是来自网络
> - 你将`MsgTransferLeader`消息的`Message.from`设置为被转移者（即转移目标）
> - 要立即开始新的选举，你可以调用`Raft.Step`并传入`MsgHup`消息
> - 调用`pb.ConfChange.Marshal`获取`pb.ConfChange`的字节表示，并将其放入`pb.Entry.Data`

## 第二部分

由于Raft模块现在支持成员变更和领导权变更，在这一部分中，你需要使TinyKV支持基于第一部分的这些管理命令。正如你在`proto/proto/raft_cmdpb.proto`中看到的，有四种类型的管理命令：

- CompactLog（已在项目2的第三部分实现）
- TransferLeader
- ChangePeer
- Split

`TransferLeader`和`ChangePeer`是基于Raft的领导权变更和成员变更支持的命令。这些将作为平衡调度器的基本操作步骤。`Split`将一个Region分裂为两个Region，这是多raft的基础。你将逐步实现它们。

### 代码

所有的修改都基于项目2的实现，因此你需要修改的代码主要集中在`kv/raftstore/peer_msg_handler.go`和`kv/raftstore/peer.go`中。

### 提议转移领导权

这一步非常简单。作为一个raft命令，`TransferLeader`将被提议为一个Raft条目。但`TransferLeader`实际上是一个不需要复制到其他节点的操作，因此你只需要为`TransferLeader`命令调用`RawNode`的`TransferLeader()`方法，而不是`Propose()`。

### 在raftstore中实现配置变更

配置变更有两种不同的类型，`AddNode`和`RemoveNode`。顾名思义，它向Region中添加或删除一个Peer。要实现配置变更，你应该首先学习`RegionEpoch`的术语。`RegionEpoch`是`metapb.Region`元信息的一部分。当一个Region添加或删除Peer或分裂时，Region的epoch会发生变化。RegionEpoch的`conf_ver`在配置变更时增加，而`version`在分裂时增加。它用于在网络隔离的情况下保证最新的region信息，即一个Region中的两个领导者。

你需要使raftstore支持处理配置变更命令。流程如下：

1. 通过`ProposeConfChange`提议配置变更管理命令
2. 在日志提交后，更改`RegionLocalState`，包括`RegionEpoch`和`Region`中的`Peers`
3. 调用`raft.RawNode`的`ApplyConfChange()`

> 提示：
>
> - 对于执行`AddNode`，新添加的Peer将由领导者的心跳创建，检查`storeWorker`的`maybeCreatePeer()`。此时，这个Peer是未初始化的，我们不知道其Region的任何信息，因此我们使用0初始化其`Log Term`和`Index`。领导者随后会知道这个Follower没有数据（从0到5存在日志间隔），并直接向其发送快照。
> - 对于执行`RemoveNode`，你应该显式调用`destroyPeer()`来停止Raft模块。销毁逻辑已为你提供。
> - 不要忘记更新`GlobalContext`中`storeMeta`的region状态
> - 测试代码会多次调度一个配置变更的命令，直到配置变更被应用，因此你需要考虑如何忽略相同配置变更的重复命令。

### 在raftstore中实现region分裂

![raft_group](imgs/keyspace.png)

为了支持多raft，系统执行数据分片，并使每个Raft组只存储一部分数据。哈希和范围是常用的数据分片方法。TinyKV使用范围，主要原因是范围可以更好地聚合具有相同前缀的键，这对于扫描等操作非常方便。此外，范围在分裂方面优于哈希。通常，它只涉及元数据修改，不需要移动数据。

``` protobuf
message Region {
 uint64 id = 1;
 // Region键范围 [start_key, end_key)。
 bytes start_key = 2;
 bytes end_key = 3;
 RegionEpoch region_epoch = 4;
 repeated Peer peers = 5
}
```

让我们重新看一下Region的定义，它包括两个字段`start_key`和`end_key`，用于指示Region负责的数据范围。因此，分裂是支持多raft的关键步骤。最初，只有一个Region，范围是[“”， “”）。你可以将键空间视为一个循环，因此[“”， “”）表示整个空间。随着数据的写入，分裂检查器会每隔`cfg.SplitRegionCheckTickInterval`检查region的大小，并在可能的情况下生成一个分裂键以将Region切成两部分，你可以在`kv/raftstore/runner/split_check.go`中查看逻辑。分裂键将包装为`MsgSplitRegion`，由`onPrepareSplitRegion()`处理。

为了确保新创建的Region和Peer的id是唯一的，id由调度器分配。这也是提供的，因此你不需要实现它。`onPrepareSplitRegion()`实际上为pd worker调度一个任务，以向调度器请求id。并在收到调度器的响应后生成一个分裂管理命令，参见`kv/raftstore/runner/scheduler_task.go`中的`onAskSplit()`。

因此，你的任务是实现处理分裂管理命令的过程，就像配置变更一样。提供的框架支持多raft，参见`kv/raftstore/router.go`。当一个Region分裂为两个Region时，其中一个Region将继承分裂前的元数据，并只修改其范围和RegionEpoch，而另一个将创建相关的元信息。

> 提示：
>
> - 这个新创建的Region的对应Peer应该由`createPeer()`创建，并注册到router.regions。并且region的信息应插入到ctx.StoreMeta的`regionRanges`中。
> - 对于网络隔离下的region分裂情况，要应用的快照可能与现有region的范围重叠。检查逻辑在`kv/raftstore/peer_msg_handler.go`的`checkSnapshot()`中。请在实现时牢记这一点，并处理这种情况。
> - 使用`engine_util.ExceedEndKey()`与region的结束键进行比较。因为当结束键等于“”时，任何键都将等于或大于“”。
> - 需要考虑更多的错误：`ErrRegionNotFound`，`ErrKeyNotInRegion`，`ErrEpochNotMatch`。

## 第三部分

如上所述，我们kv存储中的所有数据被分成多个region，每个region包含多个副本。一个问题出现了：我们应该将每个副本放在哪里？以及如何为副本找到最佳位置？谁发送之前的AddPeer和RemovePeer命令？调度器承担了这一责任。

为了做出明智的决策，调度器应该拥有整个集群的一些信息。它应该知道每个region的位置。它应该知道它们有多少键。它应该知道它们有多大……为了获取相关信息，调度器要求每个region定期向调度器发送心跳请求。你可以在`/proto/proto/schedulerpb.proto`中找到心跳请求结构`RegionHeartbeatRequest`。在收到心跳后，调度器将更新本地region信息。

同时，调度器定期检查region信息，以发现我们的TinyKV集群是否存在不平衡。例如，如果任何存储包含太多region，应将region从该存储移动到其他存储。这些命令将作为相应region的心跳请求的响应被拾取。

在这一部分中，你需要为调度器实现上述两个功能。按照我们的指南和框架，这不会太难。

### 代码

你需要修改的代码主要集中在`scheduler/server/cluster.go`和`scheduler/server/schedulers/balance_region.go`中。如上所述，当调度器收到region心跳时，它将首先更新其本地region信息。然后它将检查该region是否有待处理的命令。如果有，它将作为响应发送回去。

你只需要实现`processRegionHeartbeat`函数，在该函数中调度器更新本地信息；以及`Schedule`函数，用于region平衡调度器，在该函数中调度器扫描存储并确定是否存在不平衡以及应该移动哪个region。

### 收集region心跳

如你所见，`processRegionHeartbeat`函数的唯一参数是一个regionInfo。它包含有关此心跳发送者region的信息。调度器需要做的只是更新本地region记录。但调度器是否应该为每个心跳更新这些记录？

当然不是！有两个原因。一是当该region没有变化时，可以跳过更新。更重要的是，调度器不能信任每个心跳。特别是，如果集群在某个部分存在分区，某些节点的信息可能是错误的。

例如，一些region在分裂后重新发起选举和分裂，但另一批孤立的节点仍然通过心跳向调度器发送过时的信息。因此，对于一个region，两个节点中的任何一个都可能说它是领导者，这意味着调度器不能信任它们。

哪一个更可信？调度器应该使用`conf_ver`和`version`来确定，即`RegionEpoch`。调度器应首先比较两个节点的region版本值。如果值相同，调度器比较配置变更版本值。配置变更版本值较大的节点必须拥有更新的信息。

简单来说，你可以按以下方式组织检查流程：

1. 检查本地存储中是否存在具有相同Id的region。如果存在，并且至少有一个心跳的`conf_ver`和`version`小于它的，此心跳region是过时的

2. 如果不存在，扫描所有与其重叠的region。心跳的`conf_ver`和`version`应大于或等于所有region，否则region是过时的。

那么调度器如何确定是否可以跳过此更新？我们可以列出一些简单的条件：

* 如果新region的`version`或`conf_ver`大于原始region，则不能跳过

* 如果领导者发生变化，则不能跳过

* 如果新region或原始region有待处理的peer，则不能跳过

* 如果ApproximateSize发生变化，则不能跳过

* …

不用担心。你不需要找到一个严格的充分必要条件。冗余更新不会影响正确性。

如果调度器根据此心跳确定更新本地存储，它应该更新两件事：region树和存储状态。你可以使用`RaftCluster.core.PutRegion`更新region树，并使用`RaftCluster.core.UpdateStoreStatus`更新相关存储的状态（例如领导者数量、region数量、待处理peer数量…）。

### 实现region平衡调度器

调度器中可以运行许多不同类型的调度器，例如region平衡调度器和leader平衡调度器。本学习材料将重点介绍region平衡调度器。

每个调度器都应实现Scheduler接口，你可以在`/scheduler/server/schedule/scheduler.go`中找到它。调度器将使用`GetMinInterval`的返回值作为默认间隔，定期运行`Schedule`方法。如果它返回null（经过几次重试），调度器将使用`GetNextInterval`增加间隔。通过定义`GetNextInterval`，你可以定义间隔如何增加。如果它返回一个操作符，调度器将将这些操作符作为相关region的下一个心跳的响应分发。

Scheduler接口的核心部分是`Schedule`方法。该方法的返回值是`Operator`，它包含多个步骤，例如`AddPeer`和`RemovePeer`。例如，`MovePeer`可能包含`AddPeer`、`transferLeader`和`RemovePeer`，这些你已经在之前的部分实现了。以下图为例，调度器尝试将peer从第三个存储移动到第四个存储。首先，它应该为第四个存储`AddPeer`。然后它检查第三个是否是领导者，发现不是，因此不需要`transferLeader`。然后它删除第三个存储中的peer。

你可以使用`scheduler/server/schedule/operator`包中的`CreateMovePeerOperator`函数创建一个`MovePeer`操作符。

![balance](imgs/balance1.png)

![balance](imgs/balance2.png)

在这一部分中，你唯一需要实现的是`scheduler/server/schedulers/balance_region.go`中的`Schedule`方法。该调度器避免在一个存储中有太多region。首先，调度器将选择所有合适的存储。然后根据它们的region大小进行排序。然后调度器尝试从region大小最大的存储中找出要移动的region。

调度器将尝试在存储中找到最适合移动的region。首先，它将尝试选择一个待处理的region，因为待处理可能意味着磁盘过载。如果没有待处理的region，它将尝试找到一个follower region。如果仍然无法选择一个region，它将尝试选择leader region。最后，它将选择要移动的region，或者调度器将尝试下一个region大小较小的存储，直到所有存储都被尝试过。

在你选择一个要移动的region后，调度器将选择一个存储作为目标。实际上，调度器将选择region大小最小的存储。然后调度器将判断此移动是否有价值，通过检查原始存储和目标存储的region大小差异。如果差异足够大，调度器应该在目标存储上分配一个新的peer，并创建一个移动peer操作符。

正如你可能已经注意到的，上述流程只是一个粗略的过程。很多问题被留了下来：

* 哪些存储适合移动？

简而言之，一个合适的存储应该是启动的，并且停机时间不能长于集群的`MaxStoreDownTime`，你可以通过`cluster.GetMaxStoreDownTime()`获取。

* 如何选择region？

调度器框架提供了三种获取region的方法。`GetPendingRegionsWithLock`，`GetFollowersWithLock`和`GetLeadersWithLock`。调度器可以从它们中获取相关region。然后你可以随机选择一个region。

* 如何判断此操作是否有价值？

如果原始存储和目标存储的region大小差异太小，在我们将region从原始存储移动到目标存储后，调度器可能下次又想移动回去。因此我们必须确保差异必须大于region的近似大小的两倍，这确保了移动后，目标存储的region大小仍然小于原始存储。